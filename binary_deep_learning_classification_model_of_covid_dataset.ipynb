{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "deep_learning_model_of_diabetes_dataset.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.8.10 64-bit ('tensorflow_2': conda)"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.10"
    },
    "interpreter": {
      "hash": "5e5088bb8a5e8f0457f6ed5a0b1def108b478b0b488ad5c9f43e1b7d6a86daf8"
    }
  },
  "cells": [
    {
      "source": [
        "<h1><center><span style=\"color:blue\">DEEP LEARNING MODEL OF COSWARA PROJECT'S COVID-19 DATASET</span></center></h1>\n",
        "\n",
        "<p>\n",
        "This work conforms to a assignment whose objective is to build a deep learning model using the Coswara project's Covid-19 dataset. I have focussed mainly on data analysis and data processing rather than building a very well performing (properly tuned) deep learning model solely to confirm the repeated statement (by many data scientists) that a well studied and processed dataset can often override the requirement of hypertuning of machine learning models. \n",
        "</p>"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "COGLoz8__RWP"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bkvbkg35FEz_"
      },
      "source": [
        "## I. Description\n",
        "\n",
        "Project [Coswara](https://github.com/iiscleap/Coswara-Data) by Indian Institute of Science (IISc) Bangalore is an attempt to build a diagnostic tool for Covid-19 based on respiratory, cough and speech sounds. The project is in the data collection stage now. It requires the participants to provide a recording of breathing sounds, cough sounds, sustained phonation of vowel sounds and a counting exercise. \n",
        "\n",
        "**NOTE:** This repository contains the raw audio data received at [https://coswara.iisc.ac.in/](https://coswara.iisc.ac.in/). \n",
        "\n",
        "The annotation process of this is ongoing on [GitHub](https://github.com/iiscleap/Coswara-Exp) and would be delayed compared to the uploaded data here. The data repository for Project Coswara can be foudn at this [link](https://coswara.iisc.ac.in/). To view more information about the database such as distributions of gender, age, etc. [click here](https://iiscleap.github.io/coswara-blog/coswara/2020/11/23/visualize_coswara_data_metadata.html)\n",
        "\n",
        "Each folder contains metadata and recordings corresponding to a person. To download and extract the data, you can run the script extract_data.py. Voice samples collected include breathing sounds (fast and slow), cough sounds (deep and shallow), phonation of sustained vowels (/a/ as in made, /i/,/o/), and counting numbers at slow and fast pace. Metadata information collected includes the participant's age, gender, location (country, state/ province), current health status (healthy/ exposed/ positive/recovered) and the presence of comorbidities (pre-existing medical conditions). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OD6V0zMJldOS"
      },
      "source": [
        "## II. Important Links\n",
        "\n",
        "- [Link to dataset used in this study](https://raw.githubusercontent.com/iiscleap/Coswara-Data/master/combined_data.csv)\n",
        "\n",
        "- [Google Colab notebook link showing visualizations](https://colab.research.google.com/github/iiscleap/coswara-blog/blob/master/_notebooks/2020-11-23-visualize_coswara_data_metadata.ipynb)\n",
        "\n",
        "- [Binder notebook link showing some visualizations](https://hub.gke2.mybinder.org/user/iiscleap-coswara-blog-ska67jbp/notebooks/_notebooks/2020-11-23-visualize_coswara_data_metadata.ipynb)"
      ]
    },
    {
      "source": [
        "<h2><span style=\"color:blue\">1. Initiate Data Analysis</h2>\n",
        "\n",
        "<p>\n",
        "    <ul>\n",
        "        <li>Install two essential libraries, namely, \"watermark\" and \"opendatasets\", if not already installed. Use \"watermark\" to check some essential informations.</li><br />\n",
        "        <li>Check the environment of this notebook and the version of Python interpreter running it.</li><br />\n",
        "        <li>Load all essential packages and check their versions. Then, import all necessary modules from the imported packages.</li><br />\n",
        "    </ul>\n",
        "<p>"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tz5ANuozDgFt"
      },
      "source": [
        "!pip install watermark --quiet\n",
        "# !pip install opendatasets --quiet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%load_ext watermark"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "svRYqffuDsac",
        "outputId": "02bdd619-a368-4e6c-b1b7-ed3815c0a477"
      },
      "source": [
        "%watermark -a \"Tirthankar Dutta\"\n",
        "%watermark -dhmntuz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9dvabsk-_UKs",
        "outputId": "729ae6e5-7f88-4d7a-acbd-dbaa26512da9"
      },
      "source": [
        "import os, sys, platform\n",
        "from platform import python_version\n",
        "\n",
        "env = sys.executable\n",
        "py_version = python_version()\n",
        "\n",
        "print()\n",
        "print(\"notebook env: %s\" % (env))\n",
        "print(\"python --version: %s\" % (py_version))\n",
        "print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\") \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AvTVdUkm_UR8"
      },
      "source": [
        "import numpy as np \n",
        "import pandas as pd \n",
        "import matplotlib as mpl\n",
        "import seaborn as sns \n",
        "import re, pprint\n",
        "import sklearn as skl \n",
        "import scipy as scp\n",
        "import statsmodels as sms\n",
        "import IPython as ipy\n",
        "import tensorflow as tf\n",
        "# import opendatasets as od\n",
        "import imblearn as imb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gUVu8Qaj_UWx",
        "outputId": "5c14d2a2-d824-4886-a1eb-a74caa60b952"
      },
      "source": [
        "print()\n",
        "%watermark --iversion\n",
        "print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U0CO0DXu_UZ0"
      },
      "source": [
        "from IPython.core.display import display\n",
        "from IPython import InteractiveShell\n",
        "InteractiveShell.ast_node_interactivity=\"all\"\n",
        "\n",
        "from matplotlib import pyplot as plt \n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import RobustScaler, StandardScaler, MinMaxScaler, LabelEncoder\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "# from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
        "# from imblearn.under_sampling import NearMiss\n",
        "from imblearn.combine import SMOTEENN, SMOTETomek\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.metrics import BinaryAccuracy, AUC"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "<h2><span style=\"color:blue\">2. Load Dataset</span></h2>\n",
        "\n",
        "<p>\n",
        "    <ul>\n",
        "        <li>Obtain and save dataset to disk using the \"opendatasets\" library</li><br />\n",
        "        <li>Load dataset from disk as a Pandas dataframe</li>    \n",
        "    </ul>\n",
        "</p>"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s3lMvXjg_Ucc"
      },
      "source": [
        "# data_url = \"https://raw.githubusercontent.com/iiscleap/Coswara-Data/master/combined_data.csv\"\n",
        "# dataset = od.download(data_url)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z_URCCPA_UfU",
        "outputId": "8fe07739-885f-4b5f-d727-43e2fbde2959"
      },
      "source": [
        "def get_file_path(file_name, search_path):\n",
        "    abs_file_path = []\n",
        "    for root, dir, files in os.walk(search_path):\n",
        "        if file_name in files:\n",
        "            abs_file_path.append(os.path.join(root, file_name))\n",
        "    return abs_file_path[0]\n",
        "\n",
        "\n",
        "file_name = \"combined_data.csv\"\n",
        "pwd = os.getcwd()\n",
        "file_path = get_file_path(file_name, pwd)\n",
        "\n",
        "print()\n",
        "print(\"current working directory: %s\" % (pwd))\n",
        "print()\n",
        "print(\"absolute file path: %s\" % (file_path))\n",
        "print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "MSq42er0JIPE",
        "outputId": "9ef80813-6537-44a5-ed91-309089783077"
      },
      "source": [
        "data = pd.read_csv(file_path, header=\"infer\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "<h2><span style=\"color:blue\">3. Explore and understand the dataset</span></h2>\n",
        "\n",
        "<p>\n",
        "    <ul>\n",
        "        <li>Configure Pandas display options.</li><br />\n",
        "        <li>Display dataset</li><br />\n",
        "        <li>Check dataset dimensionality</li><br />  \n",
        "        <li>Check the number of unique data types and their distributions present in the dataset</li><br />\n",
        "        <li>Identify and delete columns to delete based on the following criteria:</li><br />\n",
        "        <ol>\n",
        "            <li>Redundancy - No useful information</li><br />\n",
        "            <li>Missing values - Missing value cut-off/threshold is 90%</li><br />\n",
        "        </ol>\n",
        "        <li>Get a summary of dataset</li><br />\n",
        "        <li>Get descriptive statistics of the numerical and catagorical variables separately.</li>\n",
        "    </ul>\n",
        "</p>"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pd.set_option(\"display.precision\", 4)\n",
        "pd.set_option(\"display.max_rows\", 50)\n",
        "pd.set_option(\"display.max_columns\", 40)\n",
        "pd.set_option(\"display.max_colwidth\", None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EbuMVgZG_Uh4",
        "outputId": "0303b214-8a19-4bc0-f72f-075302eef241"
      },
      "source": [
        "print()\n",
        "print(f\"dataset dimensionality: {data.shape}\")\n",
        "print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iE4Uv8fUHy36",
        "outputId": "e0359644-09f9-40ad-be52-e2a573b47002"
      },
      "source": [
        "print()\n",
        "print(f\"unique data types and number of each unique data type in the dataset: \\n{data.dtypes.value_counts()}\")\n",
        "print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print()\n",
        "print(f\"label distribution of target variable 'covid_status':\\n{data['covid_status'].value_counts()}\")\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mvs_dict_gt_threshold = {}\n",
        "mvs_dict_le_threshold = {}\n",
        "vars_with_no_mvs = []\n",
        "for var in data.columns.to_list(): \n",
        "    missing_vals_magn = data[var].isnull().sum()\n",
        "    if missing_vals_magn > 0: \n",
        "        missing_vals_pct = (missing_vals_magn / data.shape[0]) * 100\n",
        "        if missing_vals_pct <= 90.0: \n",
        "            mvs_dict_le_threshold[var] = [missing_vals_magn, round(missing_vals_pct, 2)]\n",
        "        else:\n",
        "            mvs_dict_gt_threshold[var] = [missing_vals_magn, round(missing_vals_pct, 2)]\n",
        "    else:\n",
        "        vars_with_no_mvs.append(var)\n",
        "\n",
        "print()\n",
        "print(f\"total number of variables with missing values: \\\n",
        "    {len(mvs_dict_gt_threshold) + len(mvs_dict_le_threshold)} \\n\")\n",
        "\n",
        "print(\"****\" * 15)\n",
        "print(f\"number of variables with missing values > 90%: {len(mvs_dict_gt_threshold)}\")\n",
        "print(\"****\" * 15)\n",
        "for var, mvs_info in mvs_dict_gt_threshold.items():\n",
        "    print(f\"{var}: {mvs_info[0]}, {mvs_info[1]}%\")\n",
        "\n",
        "\n",
        "print()\n",
        "\n",
        "print(\"****\" * 15)\n",
        "print(f\"number of variables with missing values <= 90%: {len(mvs_dict_le_threshold)}\")\n",
        "print(\"****\" * 15)\n",
        "for var, mvs_info in mvs_dict_le_threshold.items():\n",
        "        print(f\"{var}: {mvs_info[0]}, {mvs_info[1]}%\")\n",
        "\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print()\n",
        "print(f\"number of variables without missing values: {len(vars_with_no_mvs)}\")\n",
        "print()\n",
        "print(f\"variables without missing values:\\n\\n{vars_with_no_mvs}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print()\n",
        "print(\"basic information of dataset:\")\n",
        "data.info()\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print()\n",
        "print(\"descriptive statistics of the numerical variables:\")\n",
        "data.describe(exclude=\"object\").transpose()\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print()\n",
        "print(\"descriptive statistics of the numerical variables:\")\n",
        "data.describe(include=\"object\").transpose()\n",
        "print()"
      ]
    },
    {
      "source": [
        "<h2><span style=\"color:blue\">4. Summary</span></h2>\n",
        "\n",
        "<p>\n",
        "    <ul>\n",
        "        <li>The dataset consists of 1895 records and 35 variables. Out of these 35 variables, 33 are of <code>object</code> data type, 1 is of <code>float64</code> data type, and 1 is of <code>int64</code> data type.</li></br />\n",
        "        <li>Out of the 35 variables, the one labeled \"covid_status\" is the \"target\" variable. The remaining 34 variables are \"feature\" variables. Analysis of the target variable \"covid_status\" reveals the following:</li><br />\n",
        "         <ol>\n",
        "            <li>The target is a highly imbalanced multiclass (7-class) variable.</li><br />\n",
        "            <li>The label of the majority class is called \"healthy\".</li><br />\n",
        "            <li>Keeping in mind our time crunch and the highly imbalanced nature of the target variable, we will reduce this imbalanced multiclass classification task to a binary classification problem.</li><br />\n",
        "            <li>To do this, we shall first relabel \"healthy\" $\\rightarrow$ \"Unaffected\" and then, group together the remaining labels/classes into a single category called \"Affected\". <span style=\"color:brown\">We shall call the obtained binary class distribution as an \"effective\" 2-class distribution.</span></li><br />\n",
        "            </ol>\n",
        "        <li>28 variables have null/missing values.</li><br />\n",
        "        <ol>\n",
        "            <li>Out of the 28 variables, 6 have missing values <= 90%. These 6 variables are, \"l_l\", \"rU\", \"smoker\", \"um\", \"cough\", \"test_status\", </li><br />\n",
        "            <li>Out of the 28 variables, 22 have missing values > 90%. These 22 variables are, \"cold\", \"ht\", \"diabetes\", \"fever\", \"asthma\", \"ihd\", \"bd\", \"st\", \"ftg\", \"mp\", \"loss_of_smell\", \"cld\", \"diarrhoea\", \"pneumonia\", \"ctScan\", \"testType\", \"test_date\", \"vacc\", \"ctScore\", others_resp\", \"others_preexist\"</li><br />\n",
        "        </ol>\n",
        "        <li>There are 7 variables, including the \"target\" variable, that do not have missing values. Out of these 7 variables, the first variable (column) labeled \"id\" is a redundant column.</li>\n",
        "    </ul>\n",
        "</p>"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "source": [
        "<h2><span style=\"color:blue\">5. First set of data preprocessing</span></h2>\n",
        "\n",
        "<p>\n",
        "    <ul>\n",
        "        <li>We will first regroup the classes of the target variable, thereby reducing a 7-class variable to a 2-class variable.</li><br />\n",
        "        <li>We will then delete 23 variables from the dataset out of which 22 variables contain missing values > 90% while the last variable is the \"id\" variable which (as mentioned) is redundant.</li><br />\n",
        "        <li>Finally, we shall perform an analysis of the variables without missing values.</li>\n",
        "    </ul>\n",
        "<p>"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "reduced_data = data.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "reduced_data['covid_status'] = reduced_data['covid_status'].replace(\n",
        "    {\n",
        "        'healthy': \"Unaffected\",\n",
        "        'no_resp_illness_exposed': \"Affected\",\n",
        "        'resp_illness_not_identified': \"Affected\",\n",
        "        'recovered_full': 'Affected', \n",
        "        'positive_mild': 'Affected',\n",
        "        'positive_asymp': 'Affected', \n",
        "        'positive_moderate': 'Affected'\n",
        "    })"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tmp_target = np.array(reduced_data.loc[:, \"covid_status\"])\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "tmp_target = label_encoder.fit_transform(tmp_target)\n",
        "print()\n",
        "print(f\"result of applying label_encoder: {label_encoder.classes_}\")\n",
        "print()\n",
        "\n",
        "reduced_data = reduced_data.drop(columns=[\"covid_status\"], axis=1)\n",
        "reduced_data['covid_status'] = tmp_target "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print()\n",
        "print(\"label distribution of encoded & regrouped 'covid_status' variable:\")\n",
        "print(reduced_data[\"covid_status\"].value_counts().sort_values(ascending=False))\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cols_to_delete = [\"id\"] + list(mvs_dict_gt_threshold.keys())\n",
        "print()\n",
        "print(f\"columns to delete: {cols_to_delete}\")\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "reduced_data = reduced_data.drop(columns=cols_to_delete, axis=1)\n",
        "\n",
        "print()\n",
        "print(f\"dimensionality of reduced dataset (after variables deletion): {reduced_data.shape}\")\n",
        "print()\n",
        "print(\"reduced dataset:\")\n",
        "reduced_data\n",
        "print()"
      ]
    },
    {
      "source": [
        "<h2><span style=\"color:blue\">6. Analysis and preprocessing of catagorical variables without missing values, including the \"target\" variable</span></h2>\n",
        "<p>\n",
        "    <ul> \n",
        "        <li>Description of the variables:</li><br />\n",
        "        <ol>\n",
        "            <li>\"ep\": Proficient in English (y/n)</li>\n",
        "            <li>\"a\":  Age</li>\n",
        "            <li>\"covid status\": Health status (eg: positive, mild, healthy, etc.)</li>\n",
        "            <li>\"g\": Gender (male/female/other)</li>\n",
        "            <li>\"l_c\": Country</li>\n",
        "            <li>\"l_s\": State</li><br />\n",
        "        </ol>\n",
        "        <li>We will work at a much less granularity of the dataset (due to time crunch) and hence, get rid of the 'l_s' variable and replace the 'l_c' variable with a new variable 'region' that will have \"India\" (primary country) and all the remaining countries grouped together into their respective continents.</li>\n",
        "    </ul>\n",
        "</p>"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "vars_with_no_mvs.remove(\"id\")\n",
        "print()\n",
        "print(\"variables without missing values after deleting 'id' variable:\")\n",
        "print(*vars_with_no_mvs, sep=\", \", end=\"\\n\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print() \n",
        "print(\"data types of the variables without missing values:\")\n",
        "reduced_data[vars_with_no_mvs].dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "reduced_data = reduced_data.drop(columns=['l_s'], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "reduced_data['l_c'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "reduced_data['region'] = reduced_data['l_c'].replace(\n",
        "    {\n",
        "        'United States': \"North America\",\n",
        "        'Canada': \"North America\",\n",
        "        'France': \"Europe\", \n",
        "        'Finland': \"Europe\", \n",
        "        'Germany': \"Europe\",\n",
        "        'China': \"Asia\", \n",
        "        'Oman': \"Asia\", \n",
        "        'Ireland': \"Europe\", \n",
        "        'Switzerland': \"Europe\", \n",
        "        'Iran': \"Asia\",\n",
        "        'Ukraine': \"Europe\", \n",
        "        'United Arab Emirates': \"Asia\", \n",
        "        'United Kingdom': \"Europe\",\n",
        "        'Netherlands The': \"Europe\", \n",
        "        'Hungary': \"Europe\", \n",
        "        'Israel': \"Europe\", \n",
        "        'Turkey': \"Europe\", \n",
        "        'Singapore': \"Asia\",\n",
        "        'Qatar': \"Asia\", \n",
        "        'Saudi Arabia': \"Asia\", \n",
        "        'Mexico': \"North America\", \n",
        "        'Spain': \"Europe\", \n",
        "        'Malaysia': \"Asia\", \n",
        "        'Portugal': \"Europe\",\n",
        "        'Japan': \"Asia\", \n",
        "        'Bahrain': \"Asia\", \n",
        "        'Sri Lanka': \"Asia\", \n",
        "        'Philippines': \"Asia\", \n",
        "        'Argentina': \"South America\",\n",
        "        'Brazil': \"South America\", \n",
        "        'Indonesia': \"Asia\", \n",
        "        'Ecuador': \"South America\", \n",
        "        'Italy': \"Asia\", \n",
        "        'Korea South': \"Asia\",\n",
        "        'Belgium': \"Europe\", \n",
        "        'Sweden': \"Europe\", \n",
        "        'Norway': \"Europe\", \n",
        "        'Romania': \"Europe\", \n",
        "        'Iraq': \"Asia\", \n",
        "        'Syria': \"Asia\",\n",
        "        'Russia': \"Europe\", \n",
        "        'Vietnam': \"Asia\", \n",
        "        'Bangladesh': \"Asia\", \n",
        "        'Greece': \"Europe\", \n",
        "        'Egypt': \"Asia\"\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "reduced_data = reduced_data.drop(columns=['l_c'], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "reduced_data['region'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "reduced_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.style.use(\"seaborn-ticks\")\n",
        "plt.rcParams[\"font.size\"] = 12.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(2, 2, figsize=(15,13), dpi=100)\n",
        "\n",
        "ax1 = axes[0][0]\n",
        "sns.countplot(reduced_data[\"covid_status\"], \\\n",
        "    palette=\"YlGnBu\", ax=ax1)\n",
        "ax1.set_title(\"Class/label distribution of covid status\", \\\n",
        "    fontsize=18)\n",
        "ax1.set_xlabel('Labels', fontsize = 15)\n",
        "ax1.set_ylabel('Label Distribution (weight)', fontsize = 15)\n",
        "x_offset = -0.1; y_offset = -207.5\n",
        "for p in ax1.patches:\n",
        "    b = p.get_bbox()\n",
        "    val = \"{:0.2f}\".format(b.y1 + b.y0)        \n",
        "    ax1.annotate(val, ((b.x0 + b.x1)/2 + x_offset, b.y1 + y_offset), fontsize=14)\n",
        "\n",
        "ax2 = axes[0][1]\n",
        "sns.histplot(reduced_data[\"a\"], kde=True, palette=\"YlGnBu\", ax=ax2)\n",
        "ax2.set_title(\"Histogram plot of age\", fontsize=18)\n",
        "ax2.set_xlabel('Age', fontsize = 15)\n",
        "ax2.set_ylabel('Distribution', fontsize = 15)\n",
        "\n",
        "ax3 = axes[1][0]\n",
        "sns.barplot(x=reduced_data[\"covid_status\"], y=reduced_data[\"a\"], \\\n",
        "    palette=\"YlGnBu\", ci=1.0, ax=ax3)\n",
        "ax3.set_title(\"Variation in age vs covid status\", fontsize=18)\n",
        "ax3.set_xlabel('Covid Status', fontsize = 15)\n",
        "ax3.set_ylabel('Age', fontsize = 15)\n",
        "x_offset = -0.1; y_offset = -20.5\n",
        "for p in ax3.patches:\n",
        "    b = p.get_bbox()\n",
        "    val = \"{:0.2f}\".format(b.y1 + b.y0)        \n",
        "    ax3.annotate(val, ((b.x0 + b.x1)/2 + x_offset, b.y1 + y_offset), fontsize=14)\n",
        "\n",
        "ax4 = axes[1][1]\n",
        "sns.boxplot(x=reduced_data[\"a\"], palette=\"YlGnBu\", ax=ax4)\n",
        "ax4.set_title(\"Detection of outliers in age variable using boxplot\", fontsize=18)\n",
        "ax4.set_xlabel('Values of age variable', fontsize = 15)\n",
        "ax4.set_ylabel('Age', fontsize = 15)\n",
        "\n",
        "plt.show();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(3, 1, figsize=(15,25), dpi=100)\n",
        "\n",
        "ax1 = axes[0]\n",
        "sns.countplot(reduced_data[\"ep\"], hue=reduced_data[\"covid_status\"], \\\n",
        "    palette=\"YlGnBu\", ax=ax1)\n",
        "ax1.set_title(\"Variation in covid status vs English proficiency\", fontsize=18)\n",
        "ax1.set_xlabel('English Proficiency', fontsize = 15)\n",
        "ax1.set_ylabel('Num. of Records', fontsize = 15)\n",
        "ax1.legend(fontsize = 12, title=\"Covid Status\", title_fontsize = 12)\n",
        "# ax1.legend(fontsize = 12, bbox_to_anchor= (1.26, 1.02), title=\"Covid Status\", title_fontsize = 12)\n",
        "x_offset = -0.1; y_offset = 20.5\n",
        "for p in ax1.patches:\n",
        "    b = p.get_bbox()\n",
        "    val = \"{:0.1f}\".format(b.y1 + b.y0)        \n",
        "    ax1.annotate(val, ((b.x0 + b.x1)/2 + x_offset, b.y1 + y_offset), fontsize=14)\n",
        "\n",
        "ax2 = axes[1]\n",
        "sns.countplot(reduced_data[\"g\"], hue=reduced_data[\"covid_status\"], \\\n",
        "    palette=\"YlGnBu\", ax=ax2)\n",
        "ax2.set_title(\"Variation in Covid status vs gender\", fontsize=18)\n",
        "ax2.set_xlabel('Gender', fontsize = 15)\n",
        "ax2.set_ylabel('Num. of Records', fontsize = 15)\n",
        "ax2.legend(fontsize = 12, title=\"Covid Status\", title_fontsize = 12)\n",
        "# ax2.legend(fontsize = 12, bbox_to_anchor= (1.0, 1.02), title=\"Covid Status\", title_fontsize = 12)\n",
        "x_offset = -0.1; y_offset = 20.5\n",
        "for p in ax2.patches:\n",
        "    b = p.get_bbox()\n",
        "    val = \"{:0.1f}\".format(b.y1 + b.y0)        \n",
        "    ax2.annotate(val, ((b.x0 + b.x1)/2 + x_offset, b.y1 + y_offset), fontsize=14)\n",
        "\n",
        "ax3 = axes[2]\n",
        "sns.countplot(reduced_data[\"region\"], hue=reduced_data[\"covid_status\"], \\\n",
        "    palette=\"YlGnBu\", ax=ax3)\n",
        "ax3.set_title(\"Variation in Covid status vs region\", fontsize=18)\n",
        "ax3.set_xlabel('Region', fontsize = 15)\n",
        "ax3.set_ylabel('Num. of Records', fontsize = 15)\n",
        "ax3.legend(fontsize = 12, title=\"Covid Status\", title_fontsize = 12)\n",
        "# ax3.legend(fontsize = 12, bbox_to_anchor= (1.0, 1.02), title=\"Covid Status\", title_fontsize = 12)\n",
        "x_offset = -0.1; y_offset = 20.5\n",
        "for p in ax3.patches:\n",
        "    b = p.get_bbox()\n",
        "    val = \"{:0.1f}\".format(b.y1 + b.y0)        \n",
        "    ax3.annotate(val, ((b.x0 + b.x1)/2 + x_offset, b.y1 + y_offset), fontsize=14)\n",
        "\n",
        "plt.show();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print()\n",
        "print(f\"value of skewness in the age variable: {round(reduced_data['a'].skew(),2)}\")"
      ]
    },
    {
      "source": [
        "<h2><span style=\"color:blue\">7. Summary</span></h2>\n",
        "<p>\n",
        "    <ol> \n",
        "       <li>The variable \"a\" (age) is moderately (0.96) positive-skewed and therefore we will not treat the outliers present in it manually. Rather we shall use <code>RobustScaler()</code> + <code>StandardScalar()</code> combination (from <code>sklearn.preprocessing</code> module) to deal with both the skewness and the outliers.</li></br />\n",
        "        <li>Effects of features like \"region\" and \"g\" (gender) on the target variable appears to be very asymmetric, as these variables suffer from high degree of class imbalance.</li>\n",
        "    </ol>\n",
        "</p>"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "source": [
        "<h2><span style=\"color:blue\">8. Analysis and preprocessing of categorical variables with missing values</span></h2>\n",
        "\n",
        "<p>\n",
        "    <ul>\n",
        "        <li>The level of granularity at we are working, the variable called 'l_l' (locality-specific records) is redundant and therefore we will first remove this.</li><br />\n",
        "        <li>Next, as the data types of all these 6 variables is <code>object</code>, we will replace the null values in all of them with \"Unknown\".</li><br />\n",
        "        <li>We will then preprocess them as precribed below:</li><br />\n",
        "        <ol>\n",
        "            <li>\"rU\": Assuming that (\"y\", \"n\") $\\Rightarrow$ (\"Yes\", \"No\"), i.e., (\"True\", \"False\") respectively, and we will change (\"y\", \"n\") $\\rightarrow$ (\"True\", \"False\").</li><br />\n",
        "            <li>\"smoker\": Assuming (\"n\", \"y\") refer to (\"No\", \"Yes\"), i.e., (\"False\", \"True\"), we will change (\"y\", \"n\") $\\rightarrow$ (\"True\", \"False\"), thereby allowing to reducing a 4-class variable to a 2-class one.</li><br />\n",
        "            <li>\"um\": We shall change (\"n\", \"y\") $\\rightarrow$ (\"False\", \"True\") in line with the above.</li><br />\n",
        "            <li>\"cough\": No preprocessing required.</li><br />\n",
        "            <li>\"test_status\": We will change (\"na\", \"p\", \"n\") $\\rightarrow$ (\"NA\", \"True\", \"False\").</li><br /> \n",
        "            </ol>\n",
        "            <li>Next, we will look at the distributions of the labels/classes of these variables.</li><br />\n",
        "            <li>Finally, we will TypeCast all the 6 variables from <code>object</code> to <code>category</code> data type and perform One-Hot Encoding using Pandas <code>get_dummies()</code> functionality.</li>\n",
        "    </ul>\n",
        "</p>"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "reduced_data = reduced_data.drop(columns=['l_l'], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cols_with_mvs = list(mvs_dict_le_threshold.keys())\n",
        "print()\n",
        "print(f\"list of variables with missing values before removal of 'l_l' variable:\\n{cols_with_mvs}\")\n",
        "print() \n",
        "\n",
        "cols_with_mvs.remove('l_l')\n",
        "print()\n",
        "print(f\"list of variables with missing values after removal of 'l_l' variable:\\n{cols_with_mvs}\")\n",
        "print() "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "reduced_data[cols_with_mvs] = reduced_data[cols_with_mvs].replace(\n",
        "    {\n",
        "        np.NaN: \"Unknown\",\n",
        "        np.nan: \"Unknown\"\n",
        "    })"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "reduced_data[ ['rU', 'smoker', 'um'] ] = reduced_data[ ['rU', 'smoker', 'um'] ].replace(\n",
        "    {\n",
        "        \"y\": \"True\",\n",
        "        \"n\": \"False\"\n",
        "    })"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "reduced_data['test_status'] = reduced_data['test_status'].replace(\n",
        "    {\n",
        "        \"na\": \"NA\",\n",
        "        \"p\": \"True\",\n",
        "        \"n\": \"False\",\n",
        "    })"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "reduced_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print()\n",
        "print(\"value/class distributions of the 6 variables:\")\n",
        "print()\n",
        "\n",
        "for var in cols_with_mvs:\n",
        "    print(f\"distribution for {var}:\\n\\n{reduced_data[var].value_counts()}\")\n",
        "    print()\n",
        "\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(5, 1, figsize=(18,35), dpi=100)\n",
        "\n",
        "ax1 = axes[0]\n",
        "sns.countplot(reduced_data['covid_status'], hue=reduced_data['rU'], \\\n",
        "    palette=\"YlGnBu\", ax=ax1)\n",
        "ax1.set_title(\"Covid Status vs Returning User\", fontsize=18)\n",
        "ax1.set_xlabel('Covid Status', fontsize = 15)\n",
        "ax1.set_ylabel('Num. of Records', fontsize = 15)\n",
        "ax1.legend(fontsize = 12, title=\"rU\", title_fontsize = 12)\n",
        "# ax1.legend(fontsize = 12, bbox_to_anchor= (1.26, 1.02), title=\"Covid Status\", title_fontsize = 12)\n",
        "x_offset = -0.05; y_offset = 5.5\n",
        "for p in ax1.patches:\n",
        "    b = p.get_bbox()\n",
        "    val = \"{:0.1f}\".format(b.y1 + b.y0)        \n",
        "    ax1.annotate(val, ((b.x0 + b.x1)/2 + x_offset, b.y1 + y_offset), fontsize=14)\n",
        "\n",
        "\n",
        "ax2 = axes[1]\n",
        "sns.countplot(reduced_data['covid_status'], hue=reduced_data['smoker'], \\\n",
        "    palette=\"YlGnBu\", ax=ax2)\n",
        "ax2.set_title(\"Covid Status vs Smoker\", fontsize=18)\n",
        "ax2.set_xlabel('Covid Status', fontsize = 15)\n",
        "ax2.set_ylabel('Num. of Records', fontsize = 15)\n",
        "ax2.legend(fontsize = 12, title=\"smoker\", title_fontsize = 12)\n",
        "# ax1.legend(fontsize = 12, bbox_to_anchor= (1.26, 1.02), title=\"Covid Status\", title_fontsize = 12)\n",
        "x_offset = -0.05; y_offset = 7.5\n",
        "for p in ax2.patches:\n",
        "    b = p.get_bbox()\n",
        "    val = \"{:0.1f}\".format(b.y1 + b.y0)        \n",
        "    ax2.annotate(val, ((b.x0 + b.x1)/2 + x_offset, b.y1 + y_offset), fontsize=14)\n",
        "\n",
        "ax3 = axes[2]\n",
        "sns.countplot(reduced_data['covid_status'], hue=reduced_data['um'], \\\n",
        "    palette=\"YlGnBu\", ax=ax3)\n",
        "ax3.set_title(\"Covid Status vs Using Mask\", fontsize=18)\n",
        "ax3.set_xlabel('Covid Status', fontsize = 15)\n",
        "ax3.set_ylabel('Num. of Records', fontsize = 15)\n",
        "ax3.legend(fontsize = 12, title=\"um\", title_fontsize = 12)\n",
        "# ax1.legend(fontsize = 12, bbox_to_anchor= (1.26, 1.02), title=\"Covid Status\", title_fontsize = 12)\n",
        "x_offset = -0.05; y_offset = 7.5\n",
        "for p in ax3.patches:\n",
        "    b = p.get_bbox()\n",
        "    val = \"{:0.1f}\".format(b.y1 + b.y0)        \n",
        "    ax3.annotate(val, ((b.x0 + b.x1)/2 + x_offset, b.y1 + y_offset), fontsize=14)\n",
        "\n",
        "ax4 = axes[3]\n",
        "sns.countplot(reduced_data['covid_status'], hue=reduced_data['cough'], \\\n",
        "    palette=\"YlGnBu\", ax=ax4)\n",
        "ax4.set_title(\"Covid Status vs Cough\", fontsize=18)\n",
        "ax4.set_xlabel('Covid Status', fontsize = 15)\n",
        "ax4.set_ylabel('Num. of Records', fontsize = 15)\n",
        "ax4.legend(fontsize = 12, title=\"cough\", title_fontsize = 12)\n",
        "# ax1.legend(fontsize = 12, bbox_to_anchor= (1.26, 1.02), title=\"Covid Status\", title_fontsize = 12)\n",
        "x_offset = -0.05; y_offset = 7.5\n",
        "for p in ax4.patches:\n",
        "    b = p.get_bbox()\n",
        "    val = \"{:0.1f}\".format(b.y1 + b.y0)        \n",
        "    ax4.annotate(val, ((b.x0 + b.x1)/2 + x_offset, b.y1 + y_offset), fontsize=14)\n",
        "\n",
        "ax5 = axes[4]\n",
        "sns.countplot(reduced_data['covid_status'], hue=reduced_data['test_status'], \\\n",
        "    palette=\"YlGnBu\", ax=ax5)\n",
        "ax5.set_title(\"Covid Status vs Test Status\", fontsize=18)\n",
        "ax5.set_xlabel('Covid Status', fontsize = 15)\n",
        "ax5.set_ylabel('Num. of Records', fontsize = 15)\n",
        "ax5.legend(fontsize = 12, title=\"test_status\", title_fontsize = 12)\n",
        "# ax1.legend(fontsize = 12, bbox_to_anchor= (1.26, 1.02), title=\"Covid Status\", title_fontsize = 12)\n",
        "x_offset = -0.05; y_offset = 7.5\n",
        "for p in ax5.patches:\n",
        "    b = p.get_bbox()\n",
        "    val = \"{:0.1f}\".format(b.y1 + b.y0)        \n",
        "    ax5.annotate(val, ((b.x0 + b.x1)/2 + x_offset, b.y1 + y_offset), fontsize=14)\n",
        "\n",
        "plt.show();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for var in cols_with_mvs:\n",
        "    reduced_data[var] = reduced_data[var].astype('category')\n",
        "    reduced_data = pd.get_dummies(reduced_data, columns=[var], drop_first=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "reduced_data"
      ]
    },
    {
      "source": [
        "<h2><span style=\"color:blue\">9. Remaining preprocessing and data partitioning</span></h2>\n",
        "\n",
        "<p>\n",
        "    <ul>\n",
        "        <li>We will now TypeCast all remaining categorical variables from <code>object</code> to <code>category</code> data type and then perform One-Hot Encoding of the categorical variables using Pandas <code>get_dummies()</code> functionality.</li><br />\n",
        "        <li>Next, scale the \"a\" (age) variable appropriately.</li><br />\n",
        "        <li>Partition dataset first into seen and unseen (holdout) datasets. Then, split the seen dataset further into a train dataset and a validation dataset. The validation dataset will be using for testing and tuning of the deep learning model while the unseen dataset will be used solely for testing the model. The partition sizes will be as follows:</li><br />\n",
        "        <ol>\n",
        "            <li>Seen:Unseen :: 85:15</li><br />\n",
        "            <li>Train:Validation :: 70:15</li><br />\n",
        "        </ol>   \n",
        "    </ul>\n",
        "</p>"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cat_vars_to_preprocess = reduced_data.select_dtypes(include=\"object\", exclude=[\"int64\", \"category\"]).columns.to_list()\n",
        "print()\n",
        "print(cat_vars_to_preprocess)\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "reduced_data[cat_vars_to_preprocess] = reduced_data[cat_vars_to_preprocess].astype('category')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for var in cat_vars_to_preprocess:\n",
        "    reduced_data = pd.get_dummies(reduced_data, columns=[var], drop_first=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "reduced_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "target = reduced_data.iloc[:, 1]\n",
        "reduced_data = reduced_data.drop(columns=['covid_status'], axis=1)\n",
        "features = reduced_data.iloc[:, :]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print()\n",
        "print(f\"features.shape: {features.shape}\")\n",
        "print(f\"target.shape: {target.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sme = SMOTEENN()\n",
        "features_sme, target_sme = sme.fit_resample(features, target)\n",
        "\n",
        "print()\n",
        "print(f\"after applying SMOTEEN technique, features.shape: {features.shape}\")\n",
        "print(f\"after applying SMOTEEN technique, target.shape: {target.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "features_sme"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "target_sme.value_counts().sort_values(ascending=False)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FiN27dQAVkiL"
      },
      "source": [
        "# robust_scaler = RobustScaler()\n",
        "# transformed_age = robust_scaler.fit_transform(reduced_data['a'].values.reshape(-1,1))\n",
        "\n",
        "# std_scaler = StandardScaler()\n",
        "# transformed_age = std_scaler.fit_transform(transformed_age.reshape(-1,1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kuzoT1CdYnZy"
      },
      "source": [
        "min_max_scaler = MinMaxScaler()\n",
        "transformed_age = min_max_scaler.fit_transform(features_sme['a'].values.reshape(-1,1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "_h114sOMXr3B",
        "outputId": "8564f4e5-c9a6-4966-88bf-e9664abcbcdb"
      },
      "source": [
        "indx_age = features_sme.columns.get_loc('a')\n",
        "reduced_data = features_sme.drop(columns=['a'], axis=1)\n",
        "features_sme.insert(indx_age, \"age\", transformed_age)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# target = np.array(reduced_data.loc[:, \"covid_status_Unaffected\"].values)\n",
        "# reduced_data = reduced_data.drop(columns=[\"covid_status_Unaffected\"], axis=1)\n",
        "# reduced_data[\"covid_status_Unaffected\"] = target\n",
        "# reduced_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# X = np.array(reduced_data.iloc[:, 0:-1])\n",
        "# y = np.array(reduced_data.iloc[:, -1:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X = np.array(features_sme)\n",
        "y = np.array(target_sme)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print()\n",
        "print(\"dimensionality of X:\", X.shape)\n",
        "print(\"dimensionality of y:\", y.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_seen, X_unseen, y_seen, y_unseen = train_test_split(X, y, \\\n",
        "    test_size=0.15, random_state=101, stratify=y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print()\n",
        "print(f\"dimensionality of X_seen: {X_seen.shape}\")\n",
        "print(f\"dimensionality of X_unseen: {X_unseen.shape}\")\n",
        "print(f\"dimensionality of y_seen: {y_seen.shape}\")\n",
        "print(f\"dimensionality of y_unseen: {y_unseen.shape}\")\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_seen_train, X_seen_valid, y_seen_train, y_seen_valid = \\\n",
        "    train_test_split(X_seen, y_seen, test_size=0.15, random_state=101, stratify=y_seen)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print()\n",
        "print(f\"dimensionality of X_seen_train: {X_seen_train.shape}\")\n",
        "print(f\"dimensionality of X_seen_valid: {X_seen_valid.shape}\")\n",
        "print(f\"dimensionality of y_seen_train: {y_seen_train.shape}\")\n",
        "print(f\"dimensionality of y_seen_valid: {y_seen_valid.shape}\")\n",
        "print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ruCqSGYXXHpR"
      },
      "source": [
        "<h2><span style=\"color:blue\">10. Deep learning analysis of seen (train) dataset using Tensorflow and Keras</span></h2>\n",
        "\n",
        "<p> \n",
        "  <ul>\n",
        "      <li>We define a fully connected dense network of four layers. The architecture of the neural network is as follows:</li><br />\n",
        "      <ol>\n",
        "          <li>The first (input) layer has 19 neurons corresponding to one neuron per feature. The activation function is <code>relu</code>.</li><br />\n",
        "          <li>The second and third, i.e., hidden layers, each has 8 neurons with activation function <code>relu</code>.</li><br />\n",
        "          <li>The last (output) layer has 1 neuron corresponding to the target variable with activation function <code>sigmoid</code>.</li><br />\n",
        "        </ol>\n",
        "        <li>No automatic (programmatic) hypertuning of the model is performed due to time crunch and nature of the assignment.</li><br />\n",
        "        <li>The aforementioned NN architecture is found to be optimal for this problem through nomimal manual hyperparameter tuning, namely, changing the number of neurons per hidden layer and studying the output.</li><br />\n",
        "        <li>Since we have reduced the multiclass classification problem to an effective binary task, hence we have used the <code>model.predict()</code> method.</li><br /> \n",
        "        <li>If however, we would have used an effective 3-class classification problem, or have stuck to the original multiclass problem, then we would have used the <code>model.predict_classes()</code> method as this predicts the classes directly.</li>\n",
        "    </ul>\n",
        "</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ******************************************\n",
        "# Keras model parameters:\n",
        "# ******************************************\n",
        "my_num_epochs = 75\n",
        "my_num_batches = 32\n",
        "my_num_runs = 10\n",
        "my_act_func_first_layer = \"relu\"\n",
        "my_act_func_inner_layers = \"relu\"\n",
        "my_act_func_last_layer = \"sigmoid\"\n",
        "my_num_feats = X.shape[1]\n",
        "my_loss_func = \"binary_crossentropy\"\n",
        "my_optimizer = \"adam\"\n",
        "\n",
        "my_auc = AUC(num_thresholds=1000, curve='PR', \\\n",
        "    summation_method='interpolation', name=\"auc\", dtype=float, \\\n",
        "        thresholds=None, multi_label=False, num_labels=None, \\\n",
        "            label_weights=None, from_logits=False)\n",
        "\n",
        "my_bin_accuracy = BinaryAccuracy(name='binary_accuracy', \\\n",
        "    dtype=None, threshold=0.5)\n",
        "\n",
        "my_metrics = [my_bin_accuracy, my_auc]"
      ]
    },
    {
      "source": [
        "<strong>Note:</strong> Check the following links to more about the metric classes used for binary classification and their respective <em>**kwargs</em><br /> \n",
        "\n",
        "  1. [AUC](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/AUC)\n",
        "\n",
        "  2. [Binary Accuracy](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/BinaryAccuracy)"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-5kn45JOae7"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "model = Sequential()\n",
        "model.add(Dense(19, input_dim=my_num_feats, \\\n",
        "    activation=my_act_func_first_layer))\n",
        "model.add(Dense(8, activation=my_act_func_inner_layers))\n",
        "model.add(Dense(8, activation=my_act_func_inner_layers))\n",
        "model.add(Dense(1, activation=my_act_func_last_layer))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.compile(loss=my_loss_func, optimizer=my_optimizer, metrics=my_metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def prepare_model(X, y, num_epochs, num_batches, \\\n",
        "    verbose_flag, metrics, num_runs):\n",
        "\n",
        "    iter = 1\n",
        "    loss = []\n",
        "    bin_acurcy = []\n",
        "    auc = []\n",
        "    while iter <= num_runs:\n",
        "        history = model.fit(X, y, epochs=num_epochs, \\\n",
        "            batch_size=num_batches, \\\n",
        "            verbose=verbose_flag, use_multiprocessing=True)           \n",
        "        res1, res2, res3 = model.evaluate(X, y, verbose=0)\n",
        "        loss.append(res1 * 100)\n",
        "        bin_acurcy.append(res2 * 100)\n",
        "        auc.append(res3 * 100)\n",
        "        iter += 1\n",
        "    \n",
        "    print()\n",
        "    avg_loss = round(np.mean(loss), 2)\n",
        "    avg_bin_acurcy = round(np.mean(bin_acurcy), 2) \n",
        "    avg_auc = round(np.mean(auc), 2)\n",
        "    print(f\"avg. loss {avg_loss} after {num_runs} runs\")\n",
        "    print(f\"avg. accuracy {avg_bin_acurcy} after {num_runs} runs\")\n",
        "    print(f\"avg. auc {avg_auc} after {num_runs} runs\")\n",
        "    print()\n",
        "\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(20,10), dpi=100)\n",
        "\n",
        "    x = [i for i in range(num_runs)]\n",
        "    \n",
        "    ax1 = axes[0]\n",
        "    sns.lineplot(x=x, y=loss, palette=\"bright\", \\\n",
        "        marker=\"o\", markersize=12, ax=ax1)\n",
        "    ax1.set_title(\"Loss vs Iterations\", fontsize=18)\n",
        "    ax1.set_xlabel(\"Iterations\", fontsize=14)\n",
        "    ax1.set_ylabel(\"Loss\", fontsize=14)\n",
        "\n",
        "    ax2 = axes[1]\n",
        "    sns.lineplot(x=x, y=bin_acurcy, palette=\"bright\", \\\n",
        "        marker=\"o\", markersize=12, ax=ax2)\n",
        "    ax2.set_title(\"Binary Accuracy vs Iterations\", fontsize=18)\n",
        "    ax2.set_xlabel(\"Iterations\", fontsize=14)\n",
        "    ax2.set_ylabel(\"Binary Accuracy\", fontsize=14)\n",
        "    \n",
        "    ax3 = axes[2]\n",
        "    sns.lineplot(x=x, y=auc, palette=\"bright\", \\\n",
        "        marker=\"o\", markersize=12, ax=ax3)\n",
        "    ax3.set_title(\"AUC vs Iterations\", fontsize=18)\n",
        "    ax3.set_xlabel(\"Iterations\", fontsize=14)\n",
        "    ax3.set_ylabel(\"AUC\", fontsize=14)\n",
        "    \n",
        "    plt.show();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def display_model_parameters():\n",
        "    print()\n",
        "    print(\"****\" * 15)\n",
        "    print(\"model parameters\")\n",
        "    print(\"****\" * 15)\n",
        "    print()\n",
        "    print(f\"number of epochs: {my_num_epochs}\")\n",
        "    print(f\"number of batches: {my_num_batches}\")\n",
        "    print(f\"number of features: {my_num_feats}\")\n",
        "    print(f\"activation function for first layer: {my_act_func_first_layer}\")\n",
        "    print(f\"activation function for inner layers: {my_act_func_inner_layers}\")\n",
        "    print(f\"activation function for outer/last layer: {my_act_func_last_layer}\")\n",
        "    print(f\"loss function minimized: {my_loss_func}\")\n",
        "    print(f\"optimizer: {my_optimizer}\")\n",
        "    print(f\"error metrics: {my_metrics}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def check_model_outputs(y_actual, y_predicted):\n",
        "    check_model = pd.DataFrame(data=y_actual, columns=['actual values'])\n",
        "    check_model['predicted values'] = y_predicted\n",
        "    return check_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def model_performance_report(model):\n",
        "    \n",
        "    display_model_parameters()\n",
        "\n",
        "    y_actual = model.iloc[:, 0]\n",
        "    y_pred = model.iloc[:, 1]\n",
        "    \n",
        "    plt.figure(figsize=(18, 12), dpi=100)\n",
        "    sns.heatmap(confusion_matrix(y_actual, y_pred), square=True, \\\n",
        "        cmap=\"YlGnBu\", linewidths=0.1, annot=True, annot_kws={\"fontsize\":18})\n",
        "    plt.show();\n",
        "\n",
        "    print(classification_report(y_actual, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# **********************************************\n",
        "# Both the following are equivalent:\n",
        "# (num_epochss, num_runs) == (150, 5) \n",
        "# (num_epochs, num_runs) == (75, 10)\n",
        "# num_batches = 10 & 32 give equivalent results \n",
        "# **********************************************\n",
        "prepare_model(X_seen_train, y_seen_train, my_num_epochs, \\\n",
        "    my_num_batches, 0, my_metrics, my_num_runs)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "odOMeYoSbCup",
        "outputId": "5949dc9e-146d-4537-c66c-13e6ebeff1bc"
      },
      "source": [
        "seen_train_loss, seen_train_accuracy, seen_train_auc = \\\n",
        "    model.evaluate(X_seen_train, y_seen_train, verbose=0)\n",
        "\n",
        "print()\n",
        "print(\"Accuracy - seen train dataset: %.2f\"% (seen_train_accuracy*100))\n",
        "print()\n",
        "print(\"AUC - seen train dataset: %.2f\"% (seen_train_auc*100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ww4GkKTwbCxX",
        "outputId": "d1d10ed6-0cad-43de-cb0b-a05371ba7075"
      },
      "source": [
        "pred_on_seen_train_dataset = model.predict(X_seen_train)\n",
        "pred_on_seen_train_dataset = [round(x[0]) for x in pred_on_seen_train_dataset]\n",
        "\n",
        "model_on_seen_train_dataset = check_model_outputs(y_seen_train, \\\n",
        "    pred_on_seen_train_dataset)\n",
        "\n",
        "model_on_seen_train_dataset.head(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_performance_report(model_on_seen_train_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "seen_valid_loss, seen_valid_accuracy, seen_valid_auc = \\\n",
        "    model.evaluate(X_seen_valid, y_seen_valid, verbose=0)\n",
        "\n",
        "print()\n",
        "print(\"Accuracy - seen valid dataset: %.2f\"% (seen_valid_accuracy*100))\n",
        "print()\n",
        "print(\"AUC - seen valid dataset: %.2f\"% (seen_valid_auc*100))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qOVvaKH-bC4u"
      },
      "source": [
        "pred_on_seen_valid_dataset = model.predict(X_seen_valid)\n",
        "pred_on_seen_valid_dataset = [round(x[0]) for x in pred_on_seen_valid_dataset]\n",
        "\n",
        "model_on_seen_valid_dataset = check_model_outputs(y_seen_valid, \\\n",
        "    pred_on_seen_valid_dataset)\n",
        "\n",
        "model_on_seen_valid_dataset.head(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xGRPP7Z2OawV"
      },
      "source": [
        "model_performance_report(model_on_seen_valid_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "unseen_loss, unseen_accuracy, unseen_auc = \\\n",
        "    model.evaluate(X_unseen, y_unseen, verbose=0)\n",
        "\n",
        "print()\n",
        "print(\"Accuracy - unseen dataset: %.2f\"% (unseen_accuracy*100))\n",
        "print()\n",
        "print(\"AUC - unseen dataset: %.2f\"% (unseen_auc*100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pred_on_unseen_dataset = model.predict(X_unseen)\n",
        "pred_on_unseen_dataset = [round(x[0]) for x in pred_on_unseen_dataset]\n",
        "\n",
        "model_on_unseen_dataset = check_model_outputs(y_unseen, \\\n",
        "    pred_on_unseen_dataset)\n",
        "\n",
        "model_on_unseen_dataset.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_performance_report(model_on_unseen_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ]
}